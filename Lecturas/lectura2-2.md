# Critica Bayesian Personalized Ranking para Feedback Implícito

En este *paper* nuevamente se expone un estudio relacionado al filtrado colaborativo (CF) basado en **feedback implícito**, sin embargo, esta vez el análisis se focalizó en el criterio de optimización de parámetros más que del modelo en sí. La implementación propuesta, consistía en obtener el máximo estimador a posteriori del análisis bayesiano del problema a resolver, este enfoque es conocido como **Bayesian Pesonalized Ranking (BPR)**, donde a su vez este estimador es calculado a partir de la técnica de descenso estocástico de gradiente para obtener su máximo valor. A partir de los experimentos realizados, los autores obtuvieron un mejor rendimiento en la predicción de modelos de factorización matricial (MF) y k vecinos cercanos (kNN) cuando a este le aplicaban una optimización BPR de sus parámetros que cuando utilizaban el método tradicional (sin optimización Bayesiana).

En lineas generales, me gustó mucho este *paper*, ya que siento que no cometía algunos de los errores comunes que noté en las otras lecturas, sobre todo cuando se trataba de la experimentación con un sólo *dataset* o *modelo*. A pesar de que el estudio es del 2009, es súper riguroso en términos del estudio, ya que cada prueba comparativa fue realizada con dos modelos distintos (kNN, MF), tres técnicas de optimización de parámetros (SVD, WR, BPR) y dos *datasets*. Lo anterior es súmamente relevante, ya que asegura de mejor forma los resultados obtenidos y evita problemas comunes en *machine learning* en términos de generalización.

Por otro lado, estoy muy de acuerdo con la "enseñanza final" que exponen los autores en la sección de conclusiones, donde explican que en términos de sistemas recomendadores, no sólo es importante el modelo elegido, sino que también el criterio de optimización de parámetros utilizado para este. Creo que es importante tenerlo en cuenta, ya que muchas veces se busca mejorar los algoritmos desde el enfoque computacional, pero acá nos exponen que siempre es bueno poner los esfuerzos en los conceptos matemáticos básicos, para tener una mejor implementación gracias a la optimalidad de parámetros.

Sin embargo, algo que no me fue cómodo de la lectura, es que se ocupaba una interpretación distinta del feedback implícito a lo que lo hacía el otro *paper* de la semana. Mientras el documento anterior establecía métricas de rating a partir de preferencias y nivel de confianza, en este se trabajaba con un rating por pares de items, donde se comparaba si uno era preferible al otro a partir de comportamientos del usuario. De todas formas, esto no quiere decir que haya una implementación mejor que la otra, simplemente que sentí que hubiese sido un mejor punto de comparación el hecho de que hubieran ocupado una misma interpretación del feedback implícito.

Por último, sentí que en muchas ocasiones los autores aseguraban que BPR era la mejor técnica de optimización de parámetros existente, donde incluso aseguraban en las conclusiones que este era el método correcto a utilizar para tareas de rankings personalizados. Creo que esto puede ser problemático, ya que el hecho de que haya tenido buenos resultados en este estudio es un buen indicio, pero no te asegura que el método funcionará para todos los casos. Sin ir más lejos, el método de descenso de gradiente (utilzado por BPR), no siempre da las garantías de encontrar el óptimo, ya que hay ejemplos mostrados en la literatura donde el algoritmo falla y no encuentra el óptimo, si no más bien puntos silla o óptimos locales.