# Crítica: Collaborative Filtering for Implicit Feedback Datasets

El *paper* de la semana fue publicado el año 2008 y exponía la implementación de un sistema recomendador poco común para la época, este algoritmo propuesto consitía en la técnica de **filtrado colaborativo (CF)** basado en **feedback implícito**.La lectura da una explicación muy clara sobre el feedback implícito, el cual consiste en determinar las preferencias del usuario a partir comportamientos específicos de este, donde a diferencia del feedback explícito, no requiere de un input directo del usuario para determinar el rating en una matriz *usuario-item*. Otro aspecto importante de la implementación mencionada, es que posee carecterísticas muy interesantes y que eventualmente la hicieron preferible sobre otros algoritmos de sistemas de recomendación, como es el caso de la escalabilidad, donde se logra una escalabilidad lineal según el tamaño del dataset y también se destaca una buena explicación al usuario de la recomendación realizada, esto porque se obtiene directamente del feedback elegido.

Vale decir también que el algoritmo propuesto esta basado en un **modelo de factores latentes**. Siento que esto es el punto más alto del *paper*, ya que la implentación toma en cuenta distintas carecterísticas propias del feedback implícito y las lleva a un modelo conocido como el **SVD**. Tomando en cuenta el desafío de establecer métricas de preferencias del usuario (donde ya no contamos con la evaluación directa de este), se proponen dos magnitudes de interpretación, donde la primera es la preferencia implícita del usuario sobre un item, pero que está condicionada por una cierta métrica de confianza, ya que al tratarse de feedback implícito no podemos estar 100% seguros de la intención del usuario.

Sin duda, los temas expuestos en la lectura son bastante interesantes y un dan un punto de partida hacia otras implementaciones o métodos que se puedan hacer. Es por lo anterior que me gustó mucho lo expuesto en la sección de discusión, ya que ellos proponen probar este mismo método (que logró buenos resultados), pero con una mayor complejidad computacional, donde se considere una métrica de confianza distinta de cero para los items que no fueron vistos por el usuario. Creo que esto es lo más lógico, ya que si se podría incluir feedback implicito incluso en los items que no han sido vistos, como por ejemplo respondiendo a la pregunta de ¿Qué tan asequible es cierto item para un determinado usuario?. De dicha manera incluso se podría llegar a mejores resultados de los vistos en este *paper*, sin embargo, también supondría otros problemas como la capacidad de cómputo o la escalabilidad.

También me hubiese gustado que se considere probar con una implementación que considere ambos feedbacks, tanto explícito como implicito. Entiendo que el trabajarlo en detalle no era el objetivo del documento, pero de igual forma creo que se pudo haber expuesto en la sección de discusión.

Por otro lado, nuevamente observé el problema que critiqué la semana pasada, donde en el estudio sólo se trabaja con un *dataset* (TV shows). Lo anterior es complicado, ya que puede limitar fuertemente la generalización del algoritmo o aumentar el *overfitting*. De igual forma, pienso que esto se debe al año de la publicación, ya que creo que hoy en día se exige que los estudios sean probados con al menos más de tres *datasets*, cosa de evitar los problemas que mencioné.

Por último, eché de menos que se comentara más sobre el tema del *sparsity*. Siento que una de las carecterísticas más importantes del feedback implícito es que se reduce notablemente el *sparsity* de la matriz usuario-item, ya que si un usuario no ha visto un item simplemente este se puede marcar con una métrica de confianza igual a cero, lo que reduce fuertemete algunos problemas evidenciados cuando se trabaja con feedback explícito. 